{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digit Classification with our own Framework\n",
    "\n",
    "Lab Assignment from [AI for Beginners Curriculum](https://github.com/microsoft/ai-for-beginners).\n",
    "\n",
    "### Reading the Dataset\n",
    "\n",
    "This code download the dataset from the repository on the internet. You can also manually copy the dataset from `/data` directory of AI Curriculum repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !rm *.pkl\n",
    "# !wget https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/data/mnist.pkl.gz\n",
    "# !gzip -d mnist.pkl.gz\n",
    "\n",
    "import urllib.request\n",
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Delete any existing mnist.pkl files\n",
    "if os.path.exists(\"mnist.pkl\"):\n",
    "    os.remove(\"mnist.pkl\")\n",
    "if os.path.exists(\"mnist.pkl.gz\"):\n",
    "    os.remove(\"mnist.pkl.gz\")\n",
    "\n",
    "# Download the file\n",
    "url = \"https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/data/mnist.pkl.gz\"\n",
    "urllib.request.urlretrieve(url, \"mnist.pkl.gz\")\n",
    "\n",
    "# Unzip it\n",
    "with gzip.open(\"mnist.pkl.gz\", \"rb\") as f_in:\n",
    "    with open(\"mnist.pkl\", \"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('mnist.pkl','rb') as f:\n",
    "    MNIST = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = 0\n",
    "labels = 1\n",
    "features = 0\n",
    "labels = MNIST[train][labels]\n",
    "data = MNIST[train][features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what is the shape of data that we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data\n",
    "\n",
    "We will use Scikit Learn to split the data between training and test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 40000, test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(data,labels,test_size=0.2)\n",
    "\n",
    "print(f\"Train samples: {len(features_train)}, test samples: {len(features_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "1. Take the framework code from the lesson and paste it into this notebook, or (even better) into a separate Python module\n",
    "1. Define and train one-layered perceptron, observing training and validation accuracy during training\n",
    "1. Try to understand if overfitting took place, and adjust layer parameters to improve accuracy\n",
    "1. Repeat previous steps for 2- and 3-layered perceptrons. Try to experiment with different activation functions between layers.\n",
    "1. Try to answer the following questions:\n",
    "    - Does the inter-layer activation function affect network performance?\n",
    "    - Do we need 2- or 3-layered network for this task?\n",
    "    - Did you experience any problems training the network? Especially as the number of layers increased.\n",
    "    - How do weights of the network behave during training? You may plot max abs value of weights vs. epoch to understand the relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear: \n",
    "    def __init__(self, n_in, n_out):\n",
    "        self.W = np.random.normal(0, 1.0/np.sqrt(n_in), (n_out, n_in))\n",
    "        self.b = np.zeros((1, n_out))\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.dot(x, self.W.T) + self.b\n",
    "    \n",
    "    def backward(self, dz):\n",
    "        dx = np.dot(dz, self.W)\n",
    "        dW = np.dot(dz.T, self.x)\n",
    "        db = dz.sum(axis=0)\n",
    "        self.dW = dW\n",
    "        self.db = db\n",
    "        return dx\n",
    "\n",
    "    def update(self, lr):\n",
    "        self.W -= lr * self.dW\n",
    "        self.b -= lr * self.db    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax: \n",
    "    def forward(self, z):\n",
    "        self.z = z\n",
    "        zmax = z.max(axis=1, keepdims=True)\n",
    "        expz = np.exp(z - zmax)\n",
    "        Z = expz.sum(axis=1, keepdims=True)\n",
    "        return expz / Z\n",
    "    \n",
    "    def backward(self, dp):\n",
    "        p = self.forward(self.z)\n",
    "        pdp = p * dp\n",
    "        return pdp - p * pdp.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    def forward(self, p, y):\n",
    "        self.p = p\n",
    "        self.y = y\n",
    "        p_of_y = p[np.arange(len(y)), y]\n",
    "        log_prob = np.log(p_of_y)\n",
    "        return -log_prob.mean()\n",
    "    \n",
    "    def backward(self, loss):\n",
    "        dlog_softmax = np.zeros_like(self.p)\n",
    "        dlog_softmax[np.arange(len(self.y)), self.y] -= 1.0/len(self.y)\n",
    "        return dlog_softmax / self.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU: \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        return dy * (self.x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLU:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.where(x > 0, x, self.alpha * x)\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        dx = np.ones_like(self.x)\n",
    "        dx[self.x < 0] = self.alpha\n",
    "        return dy * dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh: \n",
    "    def forward(self, x):\n",
    "        y = np.tanh(x)\n",
    "        self.y = y\n",
    "        return y\n",
    "\n",
    "    def backward(self, dy):\n",
    "        return (1.0 - self.y**2) * dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net: \n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add(self, l): \n",
    "        self.layers.append(l)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, z):\n",
    "        for l in self.layers[::-1]: \n",
    "            z = l.backward(z)\n",
    "        return z\n",
    "    \n",
    "    def update(self, lr):\n",
    "        for l in self.layers: \n",
    "            if 'update' in l.__dir__(): \n",
    "                l.update(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_acc(net, x, y, loss=CrossEntropyLoss()):\n",
    "    p = net.forward(x)\n",
    "    l = loss.forward(p, y)\n",
    "    pred = np.argmax(p, axis=1)\n",
    "    acc = (pred == y).mean()\n",
    "    return l, acc\n",
    "def initialLoss(net, features_train, labels_train):\n",
    "    print(\"Initial loss={}, accuracy={}: \".format(*get_loss_acc(net, features_train, labels_train)))\n",
    "\n",
    "\n",
    "# If the model isn't learning, lower the learning rate.\n",
    "# If it's very unstable or oscillating wildly, try smaller batches or a lower LR.\n",
    "def train_epoch(net, train_x, train_labels, loss=CrossEntropyLoss(), batch_size=32, lr=0.1):\n",
    "    for i in range(0, len(train_x), batch_size):\n",
    "        xb = train_x[i:i+batch_size]\n",
    "        yb = train_labels[i:i+batch_size]\n",
    "\n",
    "        p = net.forward(xb)\n",
    "        l = loss.forward(p, yb)\n",
    "        dp = loss.backward(l)\n",
    "        dx = net.backward(dp)\n",
    "        net.update(lr)\n",
    "\n",
    "def finalLoss(net, features_train, labels_train, features_test, labels_test):\n",
    "    print(\"Final loss={}, accuracy={}: \".format(*get_loss_acc(net, features_train, labels_train)))\n",
    "    print(\"Test loss={}, accuracy={}: \".format(*get_loss_acc(net, features_test, labels_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 layered perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss=2.4311445390307678, accuracy=0.102625: \n",
      "Final loss=0.341167945007791, accuracy=0.90535: \n",
      "Test loss=0.35382201750259085, accuracy=0.9015: \n"
     ]
    }
   ],
   "source": [
    "net1 = Net()\n",
    "net1.add(Linear(784, 10))\n",
    "net1.add(Softmax())\n",
    "loss = CrossEntropyLoss()\n",
    "\n",
    "initialLoss(net1, features_train, labels_train)\n",
    "train_epoch(net1, features_train, labels_train)\n",
    "finalLoss(net1, features_train, labels_train, features_test, labels_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layered perceptron (relu, leakyrelu, tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss=2.310911715212786, accuracy=0.08835: \n",
      "Final loss=0.32665982160303914, accuracy=0.907175: \n",
      "Test loss=0.34228270062080535, accuracy=0.8994: \n"
     ]
    }
   ],
   "source": [
    "net2relu = Net()\n",
    "net2relu.add(Linear(784, 10))\n",
    "net2relu.add(ReLU())\n",
    "net2relu.add(Linear(10, 10)) # NOTE: YOU'RE ONLY TAKING IN 10 POSSIBLE VALUES NOW INSTEAD OF 784\n",
    "net2relu.add(Softmax())\n",
    "loss = CrossEntropyLoss()\n",
    "\n",
    "initialLoss(net2relu, features_train, labels_train)\n",
    "train_epoch(net2relu, features_train, labels_train)\n",
    "finalLoss(net2relu, features_train, labels_train, features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss=2.296934612477449, accuracy=0.10695: \n",
      "Final loss=0.2793492259703594, accuracy=0.921875: \n",
      "Test loss=0.29432482968541596, accuracy=0.9187: \n"
     ]
    }
   ],
   "source": [
    "net2leakyrelu = Net()\n",
    "net2leakyrelu.add(Linear(784, 10))\n",
    "net2leakyrelu.add(LeakyReLU())\n",
    "net2leakyrelu.add(Linear(10, 10))\n",
    "net2leakyrelu.add(Softmax())\n",
    "loss = CrossEntropyLoss()\n",
    "\n",
    "initialLoss(net2leakyrelu, features_train, labels_train)\n",
    "train_epoch(net2leakyrelu, features_train, labels_train)\n",
    "finalLoss(net2leakyrelu, features_train, labels_train, features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss=2.4187332967435298, accuracy=0.035575: \n",
      "Final loss=0.31791654024581595, accuracy=0.911475: \n",
      "Test loss=0.33970428208762987, accuracy=0.9039: \n"
     ]
    }
   ],
   "source": [
    "net2tanh = Net()\n",
    "net2tanh.add(Linear(784, 10))\n",
    "net2tanh.add(Tanh())\n",
    "net2tanh.add(Linear(10, 10))\n",
    "net2tanh.add(Softmax())\n",
    "loss = CrossEntropyLoss()\n",
    "\n",
    "initialLoss(net2tanh, features_train, labels_train)\n",
    "train_epoch(net2tanh, features_train, labels_train)\n",
    "finalLoss(net2tanh, features_train, labels_train, features_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 layered perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss=2.3206753725149887, accuracy=0.10395: \n",
      "Final loss=0.31911663857978356, accuracy=0.9065: \n",
      "Test loss=0.3488052702104129, accuracy=0.9014: \n"
     ]
    }
   ],
   "source": [
    "net3relu = Net()\n",
    "net3relu.add(Linear(784, 10))\n",
    "net3relu.add(ReLU())\n",
    "net3relu.add(Linear(10, 10))\n",
    "net3relu.add(ReLU())\n",
    "net3relu.add(Linear(10, 10))\n",
    "net3relu.add(Softmax())\n",
    "loss = CrossEntropyLoss()\n",
    "\n",
    "initialLoss(net3relu, features_train, labels_train)\n",
    "train_epoch(net3relu, features_train, labels_train)\n",
    "finalLoss(net3relu, features_train, labels_train, features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss=2.3105634037803227, accuracy=0.1427: \n",
      "Final loss=0.3088372990306781, accuracy=0.911425: \n",
      "Test loss=0.3286176962788192, accuracy=0.9053: \n"
     ]
    }
   ],
   "source": [
    "net3leakyrelu = Net()\n",
    "net3leakyrelu.add(Linear(784, 10))\n",
    "net3leakyrelu.add(LeakyReLU())\n",
    "net3leakyrelu.add(Linear(10, 10))\n",
    "net3leakyrelu.add(LeakyReLU())\n",
    "net3leakyrelu.add(Linear(10, 10))\n",
    "net3leakyrelu.add(Softmax())\n",
    "loss = CrossEntropyLoss()\n",
    "\n",
    "initialLoss(net3leakyrelu, features_train, labels_train)\n",
    "train_epoch(net3leakyrelu, features_train, labels_train)\n",
    "finalLoss(net3leakyrelu, features_train, labels_train, features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss=2.3556883787943903, accuracy=0.102675: \n",
      "Final loss=0.3596431074976036, accuracy=0.898475: \n",
      "Test loss=0.37605853686010343, accuracy=0.8914: \n"
     ]
    }
   ],
   "source": [
    "net3tanh = Net()\n",
    "net3tanh.add(Linear(784, 10))\n",
    "net3tanh.add(Tanh())\n",
    "net3tanh.add(Linear(10, 10))\n",
    "net3tanh.add(Tanh())\n",
    "net3tanh.add(Linear(10, 10))\n",
    "net3tanh.add(Softmax())\n",
    "loss = CrossEntropyLoss()\n",
    "\n",
    "initialLoss(net3tanh, features_train, labels_train)\n",
    "train_epoch(net3tanh, features_train, labels_train)\n",
    "finalLoss(net3tanh, features_train, labels_train, features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
